# \# Transformers

# 

# This section focuses on Transformer architectures, the backbone of modern LLMs.

# 

# \## Projects

# \- \*\*Transformer from Scratch\*\* in PyTorch.

# \- \*\*BERT Fine-Tuning\*\* on classification tasks.

# \- \*\*Attention Visualization\*\*.

# 

# \## Purpose

# Illustrates understanding of self-attention, encoder/decoder structures, and contextual embeddings for NLP.

# 

